2021 March: notes about the roms grid.

useful link: https://www.myroms.org/wiki/Parallelization
The following description tries to compliment the link above, and fill in 
descriptions deemed missing from the roms wiki.


Full domain:

	The number of rho-points are given in term of xi_rho & eta_rho.
	These include an extra buffer point along each boundary.
	Therefore the internal domain size if given in LLm and MMm, such that:
	LLm = xi_rho  - 2
	MMm = eta_rho - 2

	   0 1         LLm LLm+1
	   .....      ......      MMm+1
	   :  __       __  :      MMm         DIAGRAM 1
	   : |           | :
	   : |           | :                xi_rho = LLm + 2
		                               eta_rho = MMm + 2 
		            
	   : |           | :             j ^
	   : |__       __| :       1       |
	   .....      ......       0        ---> 
		                                   i


Subdomains:

	The roms partitioning into subdomains (nodes) is based on the choice of 
	NP_XI & NP_ETA (number of processes in x & y), such that number of processes NP = NP_XI x NP_ETA
	The nodes are numbered in sequential rows starting from zero from the SW corner of domains.
	In diagram 2, we chose NP_XI=4 & NP_ETA=3, and hence we must use 12 mpi processes (cores).

	The position of the subdomain in the full domain is stored by its inode and jnode value, see diagram 3.


		DIAGRAM 2                                      DIAGRAM 3
	 MPI processes (nodes): 00-11                        inode & jnode

              ----------------                            --------------------
	     | 08  09  10  11 |                          | 0,2  1,2  2,2  3,2 |                 
	     |                |                          |                    |     
	     | 04  05  06  07 |  NP_ETA=3                | 0,1  1,1  2,1  3,1 |       
	     |                |                          |                    |                 
	     | 00  01  02  03 |                          | 0,0  1,0  2,0  3,0 | 
	      ----------------                            -------------------- 
		    NP_XI=4               
	 
	 
Subdomain size in x & y:
	 
	ROMS calculates the grid point index ranges in i/j for each MPI process (node). 
	The total grid points in x (LLm) are then divided by the number of subdomains chosen in x (NP_XI)
	to give:
	Lm - the number of grid point in x (Lm) for each subdomain, such that Lm = LLm / NP_XI.
	However, LLm may not be exactly divisible by NP_XI, in which case the subdomains cannot all have 
	the same number of grid points in the x axis.
	To account for this, the largest subdomain size is calculated in param.h using upward rounding:
	Lm = (LLm+NP_XI-1) / NP_XI

	For example, if we have a domain in x: 
	xi_rho=23
	LLm=21
	NP_XI=4  (choice made)
	then we find largest subdomain size in x:
	 6 = ( 21+  4  -1) /  4    = 24/4 = Lm     (fortran rounds down, so if say 27/4, answer would still be 6)

	ROMS is coded such that all internal nodes (not on the boundary) are the maximum subdomain size (Lm).
	In the example, nodes 01 & 02 would have 6 (Lm) grip points in x. This would mean there needs to
	be 21-2x6 = 9 grid points in x contained within boundary nodes 00 and 03.
	ROMS then splits the remaining nodes as evenly as possible in the boundary nodes, such that 00 has 5,
	and 03 has 4.
	Thus the final grid point subdomain split in x is: 5 + 6 + 6 + 4 = 21 = LLm.

	The same approach is applied for the number of grid points in y (Mm) per subdomain, where:
	Mm = (MMm+NP_ETA-1) / NP_ETA


Subdomain ranges in x/y:

	In the previous section we established a scenario where the number of grid points per subdomain in a
	direction along a row of subdomains may not all be the same.
	However, ROMS still assigns all subdomains the same sized 2D/3D arrays. E.g.:
	zeta(GLOBAL_2D_ARRAY,4) where we 'define GLOBAL_2D_ARRAY -1:Lm+2,-1:Mm+2' in set_global_definitions.h
	Note, I removed padd_X & padd_E as they are confusing and not directly relevant.
	Note, the 4 is for the various timestepping values of zeta.
	All arrays are thus sized 1-Lm with 2 additional grid points on each side: indices -1 & 0 on west side,
	and Lm+1 & Lm+2 on the east side. (This computational margin is discussed further down).
	
	To deal with the fact number of grid points are not all the same (even though the array size is the same),
	The ranges of i and j to store data in the arrays are limited to the number of grid points.
	I.e. you don't loop over the size of the array, but rather the number of grid points in the node.
	For the west boundary nodes, the arrays are filled such that the node's end grid points to the east
	populate the end of the array.
	Thus, for the grid points to be grouped, the data cannot start at the beginning of the array.
	For our example we had Lm=6, but node 00 has only 5 grid points, see diagram 4.
	
	          DIAGRAM 4:					DIAGRAM 5:
	     West boundary nodes array 			East boundary nodes array
	       
		[ ...........				      [ ...........
		  - x x x x x   (in ij plane)		        x x x x - -   (in ij plane)
		  - x x x x x 	(- empty entry)		        x x x x - -   (- empty entry)
		  ........... ]				        ........... ]
	      i=  1 2 3 4 5 6  				    i=  1 2 3 4 5 6
	            ^       ^				        ^     ^
                  iwest   ieast				      iwest ieast
              
	
	Similarly, for the east boundary nodes, the node's west grid points use the first entry of the variable's
	array, but the data might not reach the end of the array. 
	For the example, east node 03 only had 4 grid points, its arrays are filled as per diagram 5.
    
	The algorithm for this is in mpi_setup.F:
		
		Excess grid points based on largest subdomain size (Lm):
		off_xi=NP_XI*Lm-LLm    	(1)		(Off_xi is the unused grid points based 
							 on NP_XI * Lm, as actual number is LLm)
		
		Catch west boundary nodes:
		if (inode == 0)      -> iwest=1+  off_xi   /2	(west boundary nodes - e.g. 00, 04, 08)
		else                 -> iwest=1              	(all others)
		
		Catch east boundary nodes:
		if (inode < NP_XI-1) -> ieast=Lm             	(all others)
        	else 	             -> ieast=Lm-(off_xi+1)/2	(east boundary nodes - e.g. 03, 07, 11)
        							(for off_xi+1, the +1 catches fortran integer division)
        
	For our example, we calculte off_xi = 4*6-21 = 3.
	We know that node 00 has an inode value of 0 hence iwest = 1+3/2 = 1+1 = 2.  (Fortran rounds 3/2 down) 
	This iwest value is seen in diagram 4.

	I believe this method was chosen to make it easier to do MPI exchanges with neighbours, as you are always
	exchanging the end values from the west boundary into the domain, and similarly always transferring 
	to the start of the east boundary from within the domain. This is probably easier to program.

	Note, all grid point use local numbering to the subdomain, i.e. ranging from 1-Lm, 
	not relative to the global grid point number!

Looping through node arrays:

	istr, iend, jstr, jend
	compute_tile_bounds.h
	
Global indices for node grid point:	

	The original roms did not directly track global grid point numbers for each node.
	The only time it used global GPs was to add the 'partition' attribute to each result file,
	whereby the global iSW corner and global jSW corner of the node is stored. HOWEVER, these values
	start from 1, but global ij numbering should start from 0, so not quite right! 
	This is done in put_global_atts.F with:
		if (WESTERN_MPI_EDGE) 	->	ibuff(3)=iSW_corn+iwest     (2)
    	else				->	ibuff(3)=iSW_corn+iwest+1 
    	
	Where iSW_corn (which is not actually the global corner, but a shifted value) is set in 
	mpi_setup.F with:
    	iSW_corn = inode*Lm-off_xi/2		(see eq (1) for off_xi)
    	
    	For our example, we find for node 00:
    	iSW_corn =   0  *6 -  3   /2    = 0 - 1 = -1
    	
	Hence, using eq 2 we find the south west corner i index for node 00 in global numering:
    	ibuff(3) = iSW_corn+iwest = -1 + 2 = 1
    	
	This is what we had hoped since the south west corner of node 00 should start with the first
	global grid point.

Nodes computational margins:

	Any side of a node that shares a border with another node has an additional 2 grid points on that
	side in the direction of the neighbouring node. This is needed to compute values within the node
	requiring interpolation (such as upwinding) with neighbouring points.
	These computational margin points are filled by exchanging values with neighbours. This is well
	illustrated in the useful link: https://www.myroms.org/wiki/Parallelization
