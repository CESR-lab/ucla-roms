#!/bin/bash
#SBATCH --job-name="ncj_mpi50"

#       %j=job_number and %N gives nodelist output="wec_real.%j.%N.out"
#SBATCH --output="ncjoin_mpi50.%N.out"

#       If using less than 128 cores then the partition is 'shared' 
#       or 'large-shared' if extra memory is needed (only use if 'shared' failed due to memory error).
#       Note, the job charge costs 4x more for 'large-shared' than 'shared' or 'compute', use allocation wisely!
#SBATCH --partition=shared
#SBATCH --nodes=1

#       Request number of cores: (Expanse has 128 cores per node)
#       If using 'compute', you will be charged 128 x number of nodes requested! Regardless of ntasks=per-node,
#       which is very important to understand to avoid being overcharged and wasting resources.
#       For ncjoin_mpi, not recommend to go higher than 50, as diminishing returns.
#SBATCH --ntasks-per-node=50

#       Leave this at 1:
#SBATCH --cpus-per-task=1

#       Default is 1GB ram per core. Uncomment & edit to increase mem:
#       compute/shared has max of 248G of ram you can request,
#       large-shared has max of 2000G of ram you can request.
#       Note, you are charged by the larger of cores used or portion of node's memory requested!
# #SBATCH --mem=200G

#SBATCH --account=cla119
#SBATCH --export=ALL
#       Time duration requested for run:
#SBATCH -t 00:10:00

#-----------------------------------------------------------------	

# Flags needed for mvapich2:
export MV2_USE_RDMA_CM=0
export MV2_IBA_HCA=mlx5_2
export MV2 DEFAULT PORT=1

module purge
module load slurm
module load cpu/0.15.4  intel/19.1.1.217  mvapich2/2.3.4
module load netcdf-c/4.7.4
module load netcdf-fortran/4.5.3

srun --mpi=pmi2 -n 50 ncjoin_mpi 10 5 pac_his.0000.*.nc



