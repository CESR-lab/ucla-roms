#!/bin/bash
#SBATCH --job-name="ncj_mpi50"

#       %j=job_number and %N gives nodelist output="wec_real.%j.%N.out"
#SBATCH --output="ncjoin_mpi50.%j.%N.out"

#       If using less than 128 cores then the partition is 'shared' 
#       or 'large-shared' if extra memory is needed (only use if 'shared' failed due to memory error).
#       Note, the job charge costs 4x more for 'large-shared' than 'shared' or 'compute', use allocation wisely!
#SBATCH --partition=shared
#SBATCH --nodes=1

#       Request number of cores: (Expanse has 128 cores per node)
#       If using 'compute', you will be charged 128 x number of nodes requested! Regardless of ntasks=per-node,
#       which is very important to understand to avoid being overcharged and wasting resources.
#       For ncjoin_mpi, not recommend to go higher than 50, as diminishing returns.
#SBATCH --ntasks-per-node=50

#       Leave this at 1:
#SBATCH --cpus-per-task=1

#       Memory: default is 1GB on all nodes. However, you can request 2GB per core on  
#       shared/compute/debug or 15.5GB/core on large-shared at no extra cost.
#       Charged on number of cores or fraction of total memory, whichever is greater.
#       shared/compute/debug total mem=256G, large-shared total mem=2000G
#            for total memory required use: #SBATCH --mem=256G
#         or for memory per cores      use: #SBATCH --mem-per-cpu=2G

#SBATCH --account=cla119
#SBATCH --export=ALL
#       Time duration requested for run:
#SBATCH -t 00:10:00

#-----------------------------------------------------------------	

# Flags needed for mvapich2:
export MV2_USE_RDMA_CM=0
export MV2_IBA_HCA=mlx5_2
export MV2 DEFAULT PORT=1

module purge
module load slurm
module load cpu/0.15.4  intel/19.1.1.217  mvapich2/2.3.4
module load netcdf-c/4.7.4
module load netcdf-fortran/4.5.3

srun --mpi=pmi2 -n 50 ncjoin_mpi 10 5 pac_his.0000.*.nc



